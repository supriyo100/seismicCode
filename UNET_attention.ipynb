{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adaptive Efficient coordinate attention\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def custom_attention(inputs, reduction_ratio=16, num_groups=8):\n",
    "    # Efficient Channel Attention (ECA)\n",
    "    channels = inputs.shape[-1]\n",
    "    eca_avg = K.mean(inputs, axis=(1, 2))\n",
    "    eca_attention = tf.keras.layers.Conv1D(1, kernel_size=1, activation='sigmoid')(eca_avg)\n",
    "    eca_attention = K.expand_dims(eca_attention, axis=1)\n",
    "    eca_attention = K.expand_dims(eca_attention, axis=1)\n",
    "    x = inputs * eca_attention\n",
    "\n",
    "    # Coordinate Attention (CA)\n",
    "    ca_channel_avg = K.mean(x, axis=2, keepdims=True)\n",
    "    ca_channel_attention = tf.keras.layers.Dense(channels, activation='sigmoid')(ca_channel_avg)\n",
    "    ca_channel_attention = K.expand_dims(ca_channel_attention, axis=2)\n",
    "    ca_channel_attention = K.expand_dims(ca_channel_attention, axis=2)\n",
    "    x = x * ca_channel_attention\n",
    "\n",
    "    # Adaptive Attention\n",
    "    adaptive_avg = K.mean(x, axis=(1, 2))\n",
    "    adaptive_attention = tf.keras.layers.Dense(1, activation='sigmoid')(adaptive_avg)\n",
    "    adaptive_attention = K.expand_dims(adaptive_attention, axis=1)\n",
    "    adaptive_attention = K.expand_dims(adaptive_attention, axis=1)\n",
    "    x = x * adaptive_attention\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adaptive Efficient coordinate attention\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def custom_attention(inputs, reduction_ratio=16, num_groups=8):\n",
    "    # Efficient Channel Attention (ECA)\n",
    "    channels = inputs.shape[-1]\n",
    "    eca_avg = K.mean(inputs, axis=(1, 2))\n",
    "    eca_attention = tf.keras.layers.Dense(channels // reduction_ratio, activation='relu')(eca_avg)\n",
    "    eca_attention = tf.keras.layers.Dense(channels, activation='sigmoid')(eca_attention)\n",
    "    eca_attention = K.expand_dims(eca_attention, axis=1)\n",
    "    eca_attention = K.expand_dims(eca_attention, axis=1)\n",
    "    x = inputs * eca_attention\n",
    "\n",
    "    # Coordinate Attention (CA)\n",
    "    ca_channel_avg = K.mean(x, axis=2, keepdims=True)\n",
    "    ca_channel_attention = tf.keras.layers.Conv1D(filters=channels // num_groups, kernel_size=1, activation='relu')(ca_channel_avg)\n",
    "    ca_channel_attention = tf.keras.layers.Conv1D(filters=channels, kernel_size=1, activation='sigmoid')(ca_channel_attention)\n",
    "    ca_channel_attention = K.expand_dims(ca_channel_attention, axis=2)\n",
    "    ca_channel_attention = K.expand_dims(ca_channel_attention, axis=2)\n",
    "    x = x * ca_channel_attention\n",
    "\n",
    "    # Adaptive Attention\n",
    "    adaptive_avg = K.mean(x, axis=(1, 2))\n",
    "    adaptive_attention = tf.keras.layers.Dense(channels // reduction_ratio, activation='relu')(adaptive_avg)\n",
    "    adaptive_attention = tf.keras.layers.Dense(channels, activation='sigmoid')(adaptive_attention)\n",
    "    adaptive_attention = K.expand_dims(adaptive_attention, axis=1)\n",
    "    adaptive_attention = K.expand_dims(adaptive_attention, axis=1)\n",
    "    x = x * adaptive_attention\n",
    "\n",
    "    return x\n",
    "\n",
    "inputs = tf.keras.layers.Input((128, 128, 1))\n",
    "\n",
    "# Contraction path\n",
    "c1 = tf.keras.layers.Conv2D(16, (3, 3), activation='tanh', kernel_initializer='he_normal', padding='same')(inputs)\n",
    "c1 = tf.keras.layers.Dropout(0.1)(c1)\n",
    "c1 = tf.keras.layers.Conv2D(16, (3, 3), activation='tanh', kernel_initializer='he_normal', padding='same')(c1)\n",
    "p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)\n",
    "f1 = tf.keras.layers.Lambda(lambda v: tf.signal.fft2d(tf.cast(v, tf.complex64)))(p1)\n",
    "f1 = tf.keras.layers.Lambda(lambda v: tf.signal.ifft2d(tf.cast(v, tf.complex64)))(f1)\n",
    "\n",
    "c2 = tf.keras.layers.Conv2D(32, (3, 3), activation='tanh', kernel_initializer='he_normal', padding='same')(abs(f1))\n",
    "c2 = tf.keras.layers.Dropout(0.1)(c2)\n",
    "c2 = tf.keras.layers.Conv2D(32, (3, 3), activation='tanh', kernel_initializer='he_normal', padding='same')(c2)\n",
    "p2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)\n",
    "f2 = tf.keras.layers.Lambda(lambda v: tf.signal.fft2d(tf.cast(v, tf.complex64)))(p2)\n",
    "f2 = tf.keras.layers.Lambda(lambda v: tf.signal.ifft2d(tf.cast(v, tf.complex64)))(f2)\n",
    "\n",
    "\n",
    "c3 = tf.keras.layers.Conv2D(64, (3, 3), activation='tanh', kernel_initializer='he_normal', padding='same')(abs(f2))\n",
    "c3 = tf.keras.layers.Dropout(0.2)(c3)\n",
    "c3 = tf.keras.layers.Conv2D(64, (3, 3), activation='tanh', kernel_initializer='he_normal', padding='same')(c3)\n",
    "p3 = tf.keras.layers.MaxPooling2D((2, 2))(c3)\n",
    "f3 = tf.keras.layers.Lambda(lambda v: tf.signal.fft2d(tf.cast(v, tf.complex64)))(p3)\n",
    "f3 = tf.keras.layers.Lambda(lambda v: tf.signal.ifft2d(tf.cast(v, tf.complex64)))(f3)\n",
    " \n",
    "c4 = tf.keras.layers.Conv2D(128, (3, 3), activation='tanh', kernel_initializer='he_normal', padding='same')(abs(f3))\n",
    "c4 = tf.keras.layers.Dropout(0.2)(c4)\n",
    "c4 = tf.keras.layers.Conv2D(128, (3, 3), activation='tanh', kernel_initializer='he_normal', padding='same')(c4)\n",
    "p4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c4)\n",
    "f4 = tf.keras.layers.Lambda(lambda v: tf.signal.fft2d(tf.cast(v, tf.complex64)))(p4)\n",
    "f4 = tf.keras.layers.Lambda(lambda v: tf.signal.ifft2d(tf.cast(v, tf.complex64)))(f4)\n",
    " \n",
    "c5 = tf.keras.layers.Conv2D(256, (3, 3), activation='tanh', kernel_initializer='he_normal', padding='same')(abs(f4))\n",
    "c5 = tf.keras.layers.Dropout(0.3)(c5)\n",
    "c5 = tf.keras.layers.Conv2D(256, (3, 3), activation='tanh', kernel_initializer='he_normal', padding='same')(c5)\n",
    "\n",
    "# Expansive path \n",
    "u6 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n",
    "u6 = tf.keras.layers.concatenate([u6, c4])\n",
    "u6_attended = custom_attention(u6)\n",
    "c6 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6_attended)\n",
    "c6 = tf.keras.layers.Dropout(0.2)(c6)\n",
    "c6 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)\n",
    "f6 = tf.keras.layers.Lambda(lambda v: tf.signal.fft2d(tf.cast(v, tf.complex64)))(c6)\n",
    "f6 = tf.keras.layers.Lambda(lambda v: tf.signal.ifft2d(tf.cast(v, tf.complex64)))(f6)\n",
    " \n",
    "u7 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(abs(f6))\n",
    "u7 = tf.keras.layers.concatenate([u7, c3])\n",
    "u7_attended = custom_attention(u7)\n",
    "c7 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)\n",
    "c7 = tf.keras.layers.Dropout(0.2)(c7)\n",
    "c7 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)\n",
    "f7 = tf.keras.layers.Lambda(lambda v: tf.signal.fft2d(tf.cast(v,tf.complex64)))(c7)\n",
    "f7 = tf.keras.layers.Lambda(lambda v: tf.signal.ifft2d(tf.cast(v,tf.complex64)))(f7)\n",
    " \n",
    "u8 = tf.keras.layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(abs(f7))\n",
    "u8 = tf.keras.layers.concatenate([u8, c2])\n",
    "u8_attended = custom_attention(u8)\n",
    "c8 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)\n",
    "c8 = tf.keras.layers.Dropout(0.1)(c8)\n",
    "c8 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)\n",
    "f8 = tf.keras.layers.Lambda(lambda v: tf.signal.fft2d(tf.cast(v,tf.complex64)))(c8)\n",
    "f8 = tf.keras.layers.Lambda(lambda v: tf.signal.ifft2d(tf.cast(v,tf.complex64)))(f8)\n",
    " \n",
    "u9 = tf.keras.layers.Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(abs(f8))\n",
    "u9 = tf.keras.layers.concatenate([u9, c1], axis=3)\n",
    "u9_attended = custom_attention(u9)\n",
    "\n",
    "c9 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)\n",
    "c9 = tf.keras.layers.Dropout(0.1)(c9)\n",
    "c9 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)\n",
    " \n",
    "outputs = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
    "model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def custom_attention(inputs, reduction_ratio=16, num_groups=8):\n",
    "    # Efficient Channel Attention (ECA)\n",
    "    channels = inputs.shape[-1]\n",
    "    eca_avg = K.mean(inputs, axis=(1, 2))\n",
    "    eca_attention = tf.keras.layers.Dense(channels // reduction_ratio, activation='relu')(eca_avg)\n",
    "    eca_attention = tf.keras.layers.Dense(channels, activation='sigmoid')(eca_attention)\n",
    "    eca_attention = K.expand_dims(eca_attention, axis=1)\n",
    "    eca_attention = K.expand_dims(eca_attention, axis=1)\n",
    "    x = inputs * eca_attention\n",
    "\n",
    "    # Coordinate Attention (CA)\n",
    "    ca_channel_avg = K.mean(x, axis=2, keepdims=True)\n",
    "    ca_channel_attention = tf.keras.layers.Conv1D(filters=channels // num_groups, kernel_size=1, activation='relu')(ca_channel_avg)\n",
    "    ca_channel_attention = tf.keras.layers.Conv1D(filters=channels, kernel_size=1, activation='sigmoid')(ca_channel_attention)\n",
    "    ca_channel_attention = K.expand_dims(ca_channel_attention, axis=2)\n",
    "    ca_channel_attention = K.expand_dims(ca_channel_attention, axis=2)\n",
    "    x = x * ca_channel_attention\n",
    "\n",
    "    # Adaptive Attention\n",
    "    adaptive_avg = K.mean(x, axis=(1, 2))\n",
    "    adaptive_attention = tf.keras.layers.Dense(channels // reduction_ratio, activation='relu')(adaptive_avg)\n",
    "    adaptive_attention = tf.keras.layers.Dense(channels, activation='sigmoid')(adaptive_attention)\n",
    "    adaptive_attention = K.expand_dims(adaptive_attention, axis=1)\n",
    "    adaptive_attention = K.expand_dims(adaptive_attention, axis=1)\n",
    "    x = x * adaptive_attention\n",
    "\n",
    "    return x\n",
    "\n",
    "inputs = tf.keras.layers.Input((128, 128, 1))\n",
    "\n",
    "# Contraction path\n",
    "c1 = tf.keras.layers.Conv2D(16, (3, 3), activation='tanh', kernel_initializer='he_normal', padding='same')(inputs)\n",
    "c1 = tf.keras.layers.Dropout(0.2)(c1)\n",
    "c1 = tf.keras.layers.Conv2D(16, (3, 3), activation='tanh', kernel_initializer='he_normal', padding='same')(c1)\n",
    "p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "c2 = tf.keras.layers.Conv2D(32, (3, 3), activation='tanh', kernel_initializer='he_normal', padding='same')(p1)\n",
    "c2 = tf.keras.layers.Dropout(0.1)(c2)\n",
    "c2 = tf.keras.layers.Conv2D(32, (3, 3), activation='tanh', kernel_initializer='he_normal', padding='same')(c2)\n",
    "p2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "c3 = tf.keras.layers.Conv2D(64, (3, 3), activation='tanh', kernel_initializer='he_normal', padding='same')(p2)\n",
    "c3 = tf.keras.layers.Dropout(0.2)(c3)\n",
    "c3 = tf.keras.layers.Conv2D(64, (3, 3), activation='tanh', kernel_initializer='he_normal', padding='same')(c3)\n",
    "p3 = tf.keras.layers.MaxPooling2D((2, 2))(c3)\n",
    "\n",
    "c4 = tf.keras.layers.Conv2D(128, (3, 3), activation='tanh', kernel_initializer='he_normal', padding='same')(p3)\n",
    "c4 = tf.keras.layers.Dropout(0.2)(c4)\n",
    "c4 = tf.keras.layers.Conv2D(128, (3, 3), activation='tanh', kernel_initializer='he_normal', padding='same')(c4)\n",
    "p4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c4)\n",
    "\n",
    "c5 = tf.keras.layers.Conv2D(256, (3, 3), activation='tanh', kernel_initializer='he_normal', padding='same')(p4)\n",
    "c5 = tf.keras.layers.Dropout(0.3)(c5)\n",
    "c5 = tf.keras.layers.Conv2D(256, (3, 3), activation='tanh', kernel_initializer='he_normal', padding='same')(c5)\n",
    "\n",
    "# Expansive path\n",
    "u6 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n",
    "u6 = tf.keras.layers.concatenate([u6, c4])\n",
    "u6_attended = custom_attention(u6)\n",
    "c6 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6_attended)\n",
    "c6 = tf.keras.layers.Dropout(0.2)(c6)\n",
    "c6 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)\n",
    "\n",
    "u7 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n",
    "u7 = tf.keras.layers.concatenate([u7, c3], axis=3)\n",
    "u7_attended = custom_attention(u7)\n",
    "c7 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7_attended)\n",
    "c7 = tf.keras.layers.Dropout(0.2)(c7)\n",
    "c7 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)\n",
    "\n",
    "u8 = tf.keras.layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)\n",
    "u8 = tf.keras.layers.concatenate([u8, c2], axis=3)\n",
    "u8_attended = custom_attention(u8)\n",
    "c8 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8_attended)\n",
    "c8 = tf.keras.layers.Dropout(0.1)(c8)\n",
    "c8 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)\n",
    "\n",
    "u9 = tf.keras.layers.Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)\n",
    "u9 = tf.keras.layers.concatenate([u9, c1], axis=3)\n",
    "u9_attended = custom_attention(u9)\n",
    "c9 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9_attended)\n",
    "c9 = tf.keras.layers.Dropout(0.1)(c9)\n",
    "c9 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)\n",
    "\n",
    "outputs = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
    "model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_18 (InputLayer)          [(None, 128, 128, 1  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d_162 (Conv2D)            (None, 128, 128, 16  160         ['input_18[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout_97 (Dropout)           (None, 128, 128, 16  0           ['conv2d_162[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_163 (Conv2D)            (None, 128, 128, 16  2320        ['dropout_97[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d_65 (MaxPooling2D  (None, 64, 64, 16)  0           ['conv2d_163[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_164 (Conv2D)            (None, 64, 64, 32)   4640        ['max_pooling2d_65[0][0]']       \n",
      "                                                                                                  \n",
      " dropout_98 (Dropout)           (None, 64, 64, 32)   0           ['conv2d_164[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_165 (Conv2D)            (None, 64, 64, 32)   9248        ['dropout_98[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling2d_66 (MaxPooling2D  (None, 32, 32, 32)  0           ['conv2d_165[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_166 (Conv2D)            (None, 32, 32, 64)   18496       ['max_pooling2d_66[0][0]']       \n",
      "                                                                                                  \n",
      " dropout_99 (Dropout)           (None, 32, 32, 64)   0           ['conv2d_166[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_167 (Conv2D)            (None, 32, 32, 64)   36928       ['dropout_99[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling2d_67 (MaxPooling2D  (None, 16, 16, 64)  0           ['conv2d_167[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_168 (Conv2D)            (None, 16, 16, 128)  73856       ['max_pooling2d_67[0][0]']       \n",
      "                                                                                                  \n",
      " dropout_100 (Dropout)          (None, 16, 16, 128)  0           ['conv2d_168[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_169 (Conv2D)            (None, 16, 16, 128)  147584      ['dropout_100[0][0]']            \n",
      "                                                                                                  \n",
      " max_pooling2d_68 (MaxPooling2D  (None, 8, 8, 128)   0           ['conv2d_169[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_170 (Conv2D)            (None, 8, 8, 256)    295168      ['max_pooling2d_68[0][0]']       \n",
      "                                                                                                  \n",
      " dropout_101 (Dropout)          (None, 8, 8, 256)    0           ['conv2d_170[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_171 (Conv2D)            (None, 8, 8, 256)    590080      ['dropout_101[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_transpose_26 (Conv2DTra  (None, 16, 16, 128)  131200     ['conv2d_171[0][0]']             \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_16 (Concatenate)   (None, 16, 16, 256)  0           ['conv2d_transpose_26[0][0]',    \n",
      "                                                                  'conv2d_169[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_172 (Conv2D)            (None, 16, 16, 128)  295040      ['concatenate_16[0][0]']         \n",
      "                                                                                                  \n",
      " dropout_102 (Dropout)          (None, 16, 16, 128)  0           ['conv2d_172[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_173 (Conv2D)            (None, 16, 16, 128)  147584      ['dropout_102[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_transpose_27 (Conv2DTra  (None, 32, 32, 64)  32832       ['conv2d_173[0][0]']             \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_17 (Concatenate)   (None, 32, 32, 128)  0           ['conv2d_transpose_27[0][0]',    \n",
      "                                                                  'conv2d_167[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_174 (Conv2D)            (None, 32, 32, 64)   73792       ['concatenate_17[0][0]']         \n",
      "                                                                                                  \n",
      " dropout_103 (Dropout)          (None, 32, 32, 64)   0           ['conv2d_174[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_175 (Conv2D)            (None, 32, 32, 64)   36928       ['dropout_103[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_transpose_28 (Conv2DTra  (None, 64, 64, 32)  8224        ['conv2d_175[0][0]']             \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_18 (Concatenate)   (None, 64, 64, 64)   0           ['conv2d_transpose_28[0][0]',    \n",
      "                                                                  'conv2d_165[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_176 (Conv2D)            (None, 64, 64, 32)   18464       ['concatenate_18[0][0]']         \n",
      "                                                                                                  \n",
      " dropout_104 (Dropout)          (None, 64, 64, 32)   0           ['conv2d_176[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_177 (Conv2D)            (None, 64, 64, 32)   9248        ['dropout_104[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_transpose_29 (Conv2DTra  (None, 128, 128, 16  2064       ['conv2d_177[0][0]']             \n",
      " nspose)                        )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_19 (Concatenate)   (None, 128, 128, 32  0           ['conv2d_transpose_29[0][0]',    \n",
      "                                )                                 'conv2d_163[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_178 (Conv2D)            (None, 128, 128, 16  4624        ['concatenate_19[0][0]']         \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout_105 (Dropout)          (None, 128, 128, 16  0           ['conv2d_178[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_179 (Conv2D)            (None, 128, 128, 16  2320        ['dropout_105[0][0]']            \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_180 (Conv2D)            (None, 128, 128, 16  272         ['conv2d_179[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,941,072\n",
      "Trainable params: 1,941,072\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# coarse_net()\n",
    "def gelu(x):\n",
    "    cdf = 0.5 * (1.0 + tf.tanh((np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3)))))\n",
    "    return x * cdf\n",
    "\n",
    "def coarse_net():\n",
    "    # Input layer\n",
    "    inputs = tf.keras.layers.Input((128, 128, 1))\n",
    "\n",
    "    # Contraction path\n",
    "    c1 = tf.keras.layers.Conv2D(16, (3, 3), activation='tanh', kernel_initializer='he_normal', padding='same')(inputs)\n",
    "    c1 = tf.keras.layers.Dropout(0.1)(c1)\n",
    "    c1 = tf.keras.layers.Conv2D(16, (3, 3), activation='tanh', kernel_initializer='he_normal', padding='same')(c1)\n",
    "    p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "    c2 = tf.keras.layers.Conv2D(32, (3, 3), activation='tanh', kernel_initializer='he_normal', padding='same')(p1)\n",
    "    c2 = tf.keras.layers.Dropout(0.1)(c2)\n",
    "    c2 = tf.keras.layers.Conv2D(32, (3, 3), activation='tanh', kernel_initializer='he_normal', padding='same')(c2)\n",
    "    p2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "    c3 = tf.keras.layers.Conv2D(64, (3, 3), activation='tanh', kernel_initializer='he_normal', padding='same')(p2)\n",
    "    c3 = tf.keras.layers.Dropout(0.2)(c3)\n",
    "    c3 = tf.keras.layers.Conv2D(64, (3, 3), activation='tanh', kernel_initializer='he_normal', padding='same')(c3)\n",
    "    p3 = tf.keras.layers.MaxPooling2D((2, 2))(c3)\n",
    "\n",
    "    c4 = tf.keras.layers.Conv2D(128, (3, 3), activation='tanh', kernel_initializer='he_normal', padding='same')(p3)\n",
    "    c4 = tf.keras.layers.Dropout(0.2)(c4)\n",
    "    c4 = tf.keras.layers.Conv2D(128, (3, 3), activation='tanh', kernel_initializer='he_normal', padding='same')(c4)\n",
    "    p4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c4)\n",
    "\n",
    "    c5 = tf.keras.layers.Conv2D(256, (3, 3), activation='tanh', kernel_initializer='he_normal', padding='same')(p4)\n",
    "    c5 = tf.keras.layers.Dropout(0.3)(c5)\n",
    "    c5 = tf.keras.layers.Conv2D(256, (3, 3), activation='tanh', kernel_initializer='he_normal', padding='same')(c5)\n",
    "\n",
    "    # Expansive path\n",
    "    u6 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n",
    "    u6 = tf.keras.layers.concatenate([u6, c4])\n",
    "    c6 = tf.keras.layers.Conv2D(128, (3, 3), activation='gelu', kernel_initializer='he_normal', padding='same')(u6)\n",
    "    c6 = tf.keras.layers.Dropout(0.2)(c6)\n",
    "    c6 = tf.keras.layers.Conv2D(128, (3, 3), activation='gelu', kernel_initializer='he_normal', padding='same')(c6)\n",
    "\n",
    "    u7 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n",
    "    u7 = tf.keras.layers.concatenate([u7, c3])\n",
    "    c7 = tf.keras.layers.Conv2D(64, (3, 3), activation='gelu', kernel_initializer='he_normal', padding='same')(u7)\n",
    "    c7 = tf.keras.layers.Dropout(0.2)(c7)\n",
    "    c7 = tf.keras.layers.Conv2D(64, (3, 3), activation='gelu', kernel_initializer='he_normal', padding='same')(c7)\n",
    "\n",
    "    u8 = tf.keras.layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)\n",
    "    u8 = tf.keras.layers.concatenate([u8, c2])\n",
    "    c8 = tf.keras.layers.Conv2D(32, (3, 3), activation='gelu', kernel_initializer='he_normal', padding='same')(u8)\n",
    "    c8 = tf.keras.layers.Dropout(0.1)(c8)\n",
    "    c8 = tf.keras.layers.Conv2D(32, (3, 3), activation='gelu', kernel_initializer='he_normal', padding='same')(c8)\n",
    "\n",
    "    u9 = tf.keras.layers.Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)\n",
    "    u9 = tf.keras.layers.concatenate([u9, c1], axis=3)\n",
    "    c9 = tf.keras.layers.Conv2D(16, (3, 3), activation='gelu', kernel_initializer='he_normal', padding='same')(u9)\n",
    "    c9 = tf.keras.layers.Dropout(0.1)(c9)\n",
    "    c9 = tf.keras.layers.Conv2D(16, (3, 3), activation='gelu', kernel_initializer='he_normal', padding='same')(c9)\n",
    "\n",
    "    # Add another Conv2D layer with 16 filters and softmax activation\n",
    "    outputs = tf.keras.layers.Conv2D(16, (1, 1), activation='softmax')(c9)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
    "    \n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "model = coarse_net()\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_18 (InputLayer)          [(None, 128, 128, 1  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d_162 (Conv2D)            (None, 128, 128, 16  160         ['input_18[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout_97 (Dropout)           (None, 128, 128, 16  0           ['conv2d_162[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_163 (Conv2D)            (None, 128, 128, 16  2320        ['dropout_97[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d_65 (MaxPooling2D  (None, 64, 64, 16)  0           ['conv2d_163[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_164 (Conv2D)            (None, 64, 64, 32)   4640        ['max_pooling2d_65[0][0]']       \n",
      "                                                                                                  \n",
      " dropout_98 (Dropout)           (None, 64, 64, 32)   0           ['conv2d_164[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_165 (Conv2D)            (None, 64, 64, 32)   9248        ['dropout_98[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling2d_66 (MaxPooling2D  (None, 32, 32, 32)  0           ['conv2d_165[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_166 (Conv2D)            (None, 32, 32, 64)   18496       ['max_pooling2d_66[0][0]']       \n",
      "                                                                                                  \n",
      " dropout_99 (Dropout)           (None, 32, 32, 64)   0           ['conv2d_166[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_167 (Conv2D)            (None, 32, 32, 64)   36928       ['dropout_99[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling2d_67 (MaxPooling2D  (None, 16, 16, 64)  0           ['conv2d_167[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_168 (Conv2D)            (None, 16, 16, 128)  73856       ['max_pooling2d_67[0][0]']       \n",
      "                                                                                                  \n",
      " dropout_100 (Dropout)          (None, 16, 16, 128)  0           ['conv2d_168[0][0]']             \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " conv2d_169 (Conv2D)            (None, 16, 16, 128)  147584      ['dropout_100[0][0]']            \n",
      "                                                                                                  \n",
      " max_pooling2d_68 (MaxPooling2D  (None, 8, 8, 128)   0           ['conv2d_169[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_170 (Conv2D)            (None, 8, 8, 256)    295168      ['max_pooling2d_68[0][0]']       \n",
      "                                                                                                  \n",
      " dropout_101 (Dropout)          (None, 8, 8, 256)    0           ['conv2d_170[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_171 (Conv2D)            (None, 8, 8, 256)    590080      ['dropout_101[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_transpose_26 (Conv2DTra  (None, 16, 16, 128)  131200     ['conv2d_171[0][0]']             \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_16 (Concatenate)   (None, 16, 16, 256)  0           ['conv2d_transpose_26[0][0]',    \n",
      "                                                                  'conv2d_169[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_172 (Conv2D)            (None, 16, 16, 128)  295040      ['concatenate_16[0][0]']         \n",
      "                                                                                                  \n",
      " dropout_102 (Dropout)          (None, 16, 16, 128)  0           ['conv2d_172[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_173 (Conv2D)            (None, 16, 16, 128)  147584      ['dropout_102[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_transpose_27 (Conv2DTra  (None, 32, 32, 64)  32832       ['conv2d_173[0][0]']             \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_17 (Concatenate)   (None, 32, 32, 128)  0           ['conv2d_transpose_27[0][0]',    \n",
      "                                                                  'conv2d_167[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_174 (Conv2D)            (None, 32, 32, 64)   73792       ['concatenate_17[0][0]']         \n",
      "                                                                                                  \n",
      " dropout_103 (Dropout)          (None, 32, 32, 64)   0           ['conv2d_174[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_175 (Conv2D)            (None, 32, 32, 64)   36928       ['dropout_103[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_transpose_28 (Conv2DTra  (None, 64, 64, 32)  8224        ['conv2d_175[0][0]']             \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_18 (Concatenate)   (None, 64, 64, 64)   0           ['conv2d_transpose_28[0][0]',    \n",
      "                                                                  'conv2d_165[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_176 (Conv2D)            (None, 64, 64, 32)   18464       ['concatenate_18[0][0]']         \n",
      "                                                                                                  \n",
      " dropout_104 (Dropout)          (None, 64, 64, 32)   0           ['conv2d_176[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_177 (Conv2D)            (None, 64, 64, 32)   9248        ['dropout_104[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_transpose_29 (Conv2DTra  (None, 128, 128, 16  2064       ['conv2d_177[0][0]']             \n",
      " nspose)                        )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_19 (Concatenate)   (None, 128, 128, 32  0           ['conv2d_transpose_29[0][0]',    \n",
      "                                )                                 'conv2d_163[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_178 (Conv2D)            (None, 128, 128, 16  4624        ['concatenate_19[0][0]']         \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout_105 (Dropout)          (None, 128, 128, 16  0           ['conv2d_178[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_179 (Conv2D)            (None, 128, 128, 16  2320        ['dropout_105[0][0]']            \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_180 (Conv2D)            (None, 128, 128, 16  272         ['conv2d_179[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,941,072\n",
      "Trainable params: 1,941,072\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def custom_attention(inputs, reduction_ratio=16, num_groups=8):\n",
    "    # Efficient Channel Attention (ECA)\n",
    "    channels = inputs.shape[-1]\n",
    "    eca_avg = K.mean(inputs, axis=(1, 2))\n",
    "    eca_attention = tf.keras.layers.Conv1D(1, kernel_size=1, activation='sigmoid')(eca_avg)\n",
    "    eca_attention = K.expand_dims(eca_attention, axis=1)\n",
    "    eca_attention = K.expand_dims(eca_attention, axis=1)\n",
    "    x = inputs * eca_attention\n",
    "\n",
    "    # Coordinate Attention (CA)\n",
    "    ca_channel_avg = K.mean(x, axis=2, keepdims=True)\n",
    "    ca_channel_attention = tf.keras.layers.Dense(channels, activation='sigmoid')(ca_channel_avg)\n",
    "    ca_channel_attention = K.expand_dims(ca_channel_attention, axis=2)\n",
    "    ca_channel_attention = K.expand_dims(ca_channel_attention, axis=2)\n",
    "    x = x * ca_channel_attention\n",
    "\n",
    "    # Adaptive Attention\n",
    "    adaptive_avg = K.mean(x, axis=(1, 2))\n",
    "    adaptive_attention = tf.keras.layers.Dense(1, activation='sigmoid')(adaptive_avg)\n",
    "    adaptive_attention = K.expand_dims(adaptive_attention, axis=1)\n",
    "    adaptive_attention = K.expand_dims(adaptive_attention, axis=1)\n",
    "    x = x * adaptive_attention\n",
    "\n",
    "    return x\n",
    "\n",
    "def gelu(x):\n",
    "    cdf = 0.5 * (1.0 + tf.tanh((np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3)))))\n",
    "    return x * cdf\n",
    "\n",
    "def coarse_net():\n",
    "    # Input layer\n",
    "    inputs = tf.keras.layers.Input((128, 128, 1))\n",
    "\n",
    "    # Contraction path\n",
    "    c1 = tf.keras.layers.Conv2D(16, (3, 3), activation='tanh', kernel_initializer='he_normal', padding='same')(inputs)\n",
    "    c1 = tf.keras.layers.Dropout(0.1)(c1)\n",
    "    c1 = tf.keras.layers.Conv2D(16, (3, 3), activation='tanh', kernel_initializer='he_normal', padding='same')(c1)\n",
    "    p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "    c2 = tf.keras.layers.Conv2D(32, (3, 3), activation='tanh', kernel_initializer='he_normal', padding='same')(p1)\n",
    "    c2 = tf.keras.layers.Dropout(0.1)(c2)\n",
    "    c2 = tf.keras.layers.Conv2D(32, (3, 3), activation='tanh', kernel_initializer='he_normal', padding='same')(c2)\n",
    "    p2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "    c3 = tf.keras.layers.Conv2D(64, (3, 3), activation='tanh', kernel_initializer='he_normal', padding='same')(p2)\n",
    "    c3 = tf.keras.layers.Dropout(0.2)(c3)\n",
    "    c3 = tf.keras.layers.Conv2D(64, (3, 3), activation='tanh', kernel_initializer='he_normal', padding='same')(c3)\n",
    "    p3 = tf.keras.layers.MaxPooling2D((2, 2))(c3)\n",
    "\n",
    "    c4 = tf.keras.layers.Conv2D(128, (3, 3), activation='tanh', kernel_initializer='he_normal', padding='same')(p3)\n",
    "    c4 = tf.keras.layers.Dropout(0.2)(c4)\n",
    "    c4 = tf.keras.layers.Conv2D(128, (3, 3), activation='tanh', kernel_initializer='he_normal', padding='same')(c4)\n",
    "    p4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c4)\n",
    "\n",
    "    c5 = tf.keras.layers.Conv2D(256, (3, 3), activation='tanh', kernel_initializer='he_normal', padding='same')(p4)\n",
    "    c5 = tf.keras.layers.Dropout(0.3)(c5)\n",
    "    c5 = tf.keras.layers.Conv2D(256, (3, 3), activation='tanh', kernel_initializer='he_normal', padding='same')(c5)\n",
    "\n",
    "    # Expansive path\n",
    "    u6 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n",
    "    u6 = tf.keras.layers.concatenate([u6, c4])\n",
    "    u6 = custom_attention(u6)  # Apply custom attention\n",
    "    c6 = tf.keras.layers.Conv2D(128, (3, 3), activation=gelu, kernel_initializer='he_normal', padding='same')(u6)\n",
    "    c6 = tf.keras.layers.Dropout(0.2)(c6)\n",
    "    c6 = tf.keras.layers.Conv2D(128, (3, 3), activation=gelu, kernel_initializer='he_normal', padding='same')(c6)\n",
    "\n",
    "    u7 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n",
    "    u7 = tf.keras.layers.concatenate([u7, c3])\n",
    "    u7 = custom_attention(u7)  # Apply custom attention\n",
    "    c7 = tf.keras.layers.Conv2D(64, (3, 3), activation=gelu, kernel_initializer='he_normal', padding='same')(u7)\n",
    "    c7 = tf.keras.layers.Dropout(0.2)(c7)\n",
    "    c7 = tf.keras.layers.Conv2D(64, (3, 3), activation=gelu, kernel_initializer='he_normal', padding='same')(c7)\n",
    "\n",
    "    u8 = tf.keras.layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)\n",
    "    u8 = tf.keras.layers.concatenate([u8, c2])\n",
    "    u8 = custom_attention(u8)  # Apply custom attention\n",
    "    c8 = tf.keras.layers.Conv2D(32, (3, 3), activation=gelu, kernel_initializer='he_normal', padding='same')(u8_attended)\n",
    "    c8 = tf.keras.layers.Dropout(0.1)(c8)\n",
    "    c8 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)\n",
    "    \n",
    "    u9 = tf.keras.layers.Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)\n",
    "    u9 = tf.keras.layers.concatenate([u9, c1], axis=3)\n",
    "    u9_attended = custom_attention(u9)\n",
    "    c9 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9_attended)\n",
    "    c9 = tf.keras.layers.Dropout(0.1)(c9)\n",
    "    c9 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)\n",
    "    \n",
    "    outputs = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
    "    \n",
    "    # Create the model\n",
    "    model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_18 (InputLayer)          [(None, 128, 128, 1  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d_162 (Conv2D)            (None, 128, 128, 16  160         ['input_18[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout_97 (Dropout)           (None, 128, 128, 16  0           ['conv2d_162[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_163 (Conv2D)            (None, 128, 128, 16  2320        ['dropout_97[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d_65 (MaxPooling2D  (None, 64, 64, 16)  0           ['conv2d_163[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_164 (Conv2D)            (None, 64, 64, 32)   4640        ['max_pooling2d_65[0][0]']       \n",
      "                                                                                                  \n",
      " dropout_98 (Dropout)           (None, 64, 64, 32)   0           ['conv2d_164[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_165 (Conv2D)            (None, 64, 64, 32)   9248        ['dropout_98[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling2d_66 (MaxPooling2D  (None, 32, 32, 32)  0           ['conv2d_165[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_166 (Conv2D)            (None, 32, 32, 64)   18496       ['max_pooling2d_66[0][0]']       \n",
      "                                                                                                  \n",
      " dropout_99 (Dropout)           (None, 32, 32, 64)   0           ['conv2d_166[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_167 (Conv2D)            (None, 32, 32, 64)   36928       ['dropout_99[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling2d_67 (MaxPooling2D  (None, 16, 16, 64)  0           ['conv2d_167[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_168 (Conv2D)            (None, 16, 16, 128)  73856       ['max_pooling2d_67[0][0]']       \n",
      "                                                                                                  \n",
      " dropout_100 (Dropout)          (None, 16, 16, 128)  0           ['conv2d_168[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_169 (Conv2D)            (None, 16, 16, 128)  147584      ['dropout_100[0][0]']            \n",
      "                                                                                                  \n",
      " max_pooling2d_68 (MaxPooling2D  (None, 8, 8, 128)   0           ['conv2d_169[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_170 (Conv2D)            (None, 8, 8, 256)    295168      ['max_pooling2d_68[0][0]']       \n",
      "                                                                                                  \n",
      " dropout_101 (Dropout)          (None, 8, 8, 256)    0           ['conv2d_170[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_171 (Conv2D)            (None, 8, 8, 256)    590080      ['dropout_101[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_transpose_26 (Conv2DTra  (None, 16, 16, 128)  131200     ['conv2d_171[0][0]']             \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_16 (Concatenate)   (None, 16, 16, 256)  0           ['conv2d_transpose_26[0][0]',    \n",
      "                                                                  'conv2d_169[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_172 (Conv2D)            (None, 16, 16, 128)  295040      ['concatenate_16[0][0]']         \n",
      "                                                                                                  \n",
      " dropout_102 (Dropout)          (None, 16, 16, 128)  0           ['conv2d_172[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_173 (Conv2D)            (None, 16, 16, 128)  147584      ['dropout_102[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_transpose_27 (Conv2DTra  (None, 32, 32, 64)  32832       ['conv2d_173[0][0]']             \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_17 (Concatenate)   (None, 32, 32, 128)  0           ['conv2d_transpose_27[0][0]',    \n",
      "                                                                  'conv2d_167[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_174 (Conv2D)            (None, 32, 32, 64)   73792       ['concatenate_17[0][0]']         \n",
      "                                                                                                  \n",
      " dropout_103 (Dropout)          (None, 32, 32, 64)   0           ['conv2d_174[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_175 (Conv2D)            (None, 32, 32, 64)   36928       ['dropout_103[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_transpose_28 (Conv2DTra  (None, 64, 64, 32)  8224        ['conv2d_175[0][0]']             \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_18 (Concatenate)   (None, 64, 64, 64)   0           ['conv2d_transpose_28[0][0]',    \n",
      "                                                                  'conv2d_165[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_176 (Conv2D)            (None, 64, 64, 32)   18464       ['concatenate_18[0][0]']         \n",
      "                                                                                                  \n",
      " dropout_104 (Dropout)          (None, 64, 64, 32)   0           ['conv2d_176[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_177 (Conv2D)            (None, 64, 64, 32)   9248        ['dropout_104[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_transpose_29 (Conv2DTra  (None, 128, 128, 16  2064       ['conv2d_177[0][0]']             \n",
      " nspose)                        )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_19 (Concatenate)   (None, 128, 128, 32  0           ['conv2d_transpose_29[0][0]',    \n",
      "                                )                                 'conv2d_163[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_178 (Conv2D)            (None, 128, 128, 16  4624        ['concatenate_19[0][0]']         \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout_105 (Dropout)          (None, 128, 128, 16  0           ['conv2d_178[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_179 (Conv2D)            (None, 128, 128, 16  2320        ['dropout_105[0][0]']            \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_180 (Conv2D)            (None, 128, 128, 16  272         ['conv2d_179[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,941,072\n",
      "Trainable params: 1,941,072\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def custom_attention(inputs, reduction_ratio=16, num_groups=8):\n",
    "    # Efficient Channel Attention (ECA)\n",
    "    channels = inputs.shape[-1]\n",
    "    eca_avg = K.mean(inputs, axis=(1, 2))\n",
    "    eca_attention = tf.keras.layers.Conv1D(1, kernel_size=1, activation='sigmoid')(eca_avg)\n",
    "    eca_attention = K.expand_dims(eca_attention, axis=1)\n",
    "    eca_attention = K.expand_dims(eca_attention, axis=1)\n",
    "    x = inputs * eca_attention\n",
    "\n",
    "    # Coordinate Attention (CA)\n",
    "    ca_channel_avg = K.mean(x, axis=2, keepdims=True)\n",
    "    ca_channel_attention = tf.keras.layers.Dense(channels, activation='sigmoid')(ca_channel_avg)\n",
    "    ca_channel_attention = K.expand_dims(ca_channel_attention, axis=2)\n",
    "    ca_channel_attention = K.expand_dims(ca_channel_attention, axis=2)\n",
    "    x = x * ca_channel_attention\n",
    "\n",
    "    # Adaptive Attention\n",
    "    adaptive_avg = K.mean(x, axis=(1, 2))\n",
    "    adaptive_attention = tf.keras.layers.Dense(1, activation='sigmoid')(adaptive_avg)\n",
    "    adaptive_attention = K.expand_dims(adaptive_attention, axis=1)\n",
    "    adaptive_attention = K.expand_dims(adaptive_attention, axis=1)\n",
    "    x = x * adaptive_attention\n",
    "\n",
    "    return x\n",
    "\n",
    "def gelu(x):\n",
    "    cdf = 0.5 * (1.0 + tf.tanh((np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3)))))\n",
    "    return x * cdf\n",
    "\n",
    "def coarse_net():\n",
    "    # Input layer\n",
    "    inputs = tf.keras.layers.Input((128, 128, 1))\n",
    "\n",
    "    # Contraction path\n",
    "    c1 = tf.keras.layers.Conv2D(16, (3, 3), activation='tanh', kernel_initializer='he_normal', padding='same')(inputs)\n",
    "    c1 = tf.keras.layers.BatchNormalization()(c1)  # Add BatchNormalization\n",
    "    c1 = tf.keras.layers.Dropout(0.1)(c1)\n",
    "    c1 = tf.keras.layers.Conv2D(16, (3, 3), activation='tanh', kernel_initializer='he_normal', padding='same')(c1)\n",
    "    p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "    c2 = tf.keras.layers.Conv2D(32, (3, 3), activation='tanh', kernel_initializer='he_normal', padding='same')(p1)\n",
    "    c2 = tf.keras.layers.BatchNormalization()(c2)  # Add BatchNormalization\n",
    "    c2 = tf.keras.layers.Dropout(0.1)(c2)\n",
    "    c2 = tf.keras.layers.Conv2D(32, (3, 3), activation='tanh', kernel_initializer='he_normal', padding='same')(c2)\n",
    "    p2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "    c3 = tf.keras.layers.Conv2D(64, (3, 3), activation='tanh', kernel_initializer='he_normal', padding='same')(p2)\n",
    "    c3 = tf.keras.layers.BatchNormalization()(c3)  # Add BatchNormalization\n",
    "    c3 = tf.keras.layers.Dropout(0.2)(c3)\n",
    "    c3 = tf.keras.layers.Conv2D(64, (3, 3), activation='tanh', kernel_initializer='he_normal', padding='same')(c3)\n",
    "    p3 = tf.keras.layers.MaxPooling2D((2, 2))(c3)\n",
    "\n",
    "    c4 = tf.keras.layers.Conv2D(128, (3, 3), activation='tanh', kernel_initializer='he_normal', padding='same')(p3)\n",
    "    c4 = tf.keras.layers.BatchNormalization()(c4)  # Add BatchNormalization\n",
    "    c4 = tf.keras.layers.Dropout(0.2)(c4)\n",
    "    c4 = tf.keras.layers.Conv2D(128, (3, 3), activation='tanh', kernel_initializer='he_normal', padding='same')(c4)\n",
    "    p4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c4)\n",
    "\n",
    "    c5 = tf.keras.layers.Conv2D(256, (3, 3), activation='tanh', kernel_initializer='he_normal', padding='same')(p4)\n",
    "    c5 = tf.keras.layers.Dropout(0.3)(c5)\n",
    "    c5 = tf.keras.layers.Conv2D(256, (3, 3), activation='tanh', kernel_initializer='he_normal', padding='same')(c5)\n",
    "\n",
    "    # Expansive path\n",
    "    u6 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n",
    "    u6 = tf.keras.layers.concatenate([u6, c4])\n",
    "    u6 = custom_attention(u6)  # Apply custom attention\n",
    "    u6 = tf.keras.layers.GroupNormalization()(u6)  # Add BatchNormalization\n",
    "    c6 = tf.keras.layers.Conv2D(128, (3, 3), activation=gelu, kernel_initializer='he_normal', padding='same')(u6)\n",
    "    c6 = tf.keras.layers.Dropout(0.2)(c6)\n",
    "    c6 = tf.keras.layers.Conv2D(128, (3, 3), activation=gelu, kernel_initializer='he_normal', padding='same')(c6)\n",
    "\n",
    "    u7 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n",
    "    u7 = tf.keras.layers.concatenate([u7, c3])\n",
    "    u7 = custom_attention(u7)  # Apply custom attention\n",
    "    u7 = tf.keras.layers.GroupNormalization()(u7)  # Add BatchNormalization\n",
    "    c7 = tf.keras.layers.Conv2D(64, (3, 3), activation=gelu, kernel_initializer='he_normal', padding='same')(u7)\n",
    "    c7 = tf.keras.layers.Dropout(0.2)(c7)\n",
    "    c7 = tf.keras.layers.Conv2D(64, (3, 3), activation=gelu, kernel_initializer='he_normal', padding='same')(c7)\n",
    "\n",
    "    u8 = tf.keras.layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)\n",
    "    u8 = tf.keras.layers.concatenate([u8, c2])\n",
    "    u8 = custom_attention(u8)  # Apply custom attention\n",
    "    u8 = tf.keras.layers.GroupNormalization()(u8)  # Add BatchNormalization\n",
    "    c8 = tf.keras.layers.Conv2D(32, (3, 3), activation=gelu, kernel_initializer='he_normal', padding='same')(u8)\n",
    "    c8 = tf.keras.layers.Dropout(0.1)(c8)\n",
    "    c8 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)\n",
    "    \n",
    "    u9 = tf.keras.layers.Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)\n",
    "    u9 = tf.keras.layers.concatenate([u9, c1], axis=3)\n",
    "    u9_attended = custom_attention(u9)\n",
    "    u9 = tf.keras.layers.GroupNormalization()(u9)  # Add BatchNormalization\n",
    "    c9 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9_attended)\n",
    "    c9 = tf.keras.layers.Dropout(0.1)(c9)\n",
    "    c9 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)\n",
    "    \n",
    "    outputs = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
    "    \n",
    "    # Create the model\n",
    "    model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Adaptive Mixed Domain attention\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def custom_attention(inputs, reduction_ratio=16, num_groups=8):\n",
    "    # Adaptive Attention\n",
    "    adaptive_avg = K.mean(inputs, axis=(1, 2))\n",
    "    adaptive_attention = tf.keras.layers.Dense(1, activation='sigmoid')(adaptive_avg)\n",
    "    adaptive_attention = K.expand_dims(adaptive_attention, axis=1)\n",
    "    adaptive_attention = K.expand_dims(adaptive_attention, axis=1)\n",
    "    x = inputs * adaptive_attention\n",
    "\n",
    "    # Lightweight Mixed-Domain Attention\n",
    "    domain_avg = K.mean(x, axis=(1, 2))\n",
    "    domain_attention = tf.keras.layers.Dense(1, activation='sigmoid')(domain_avg)\n",
    "    domain_attention = K.expand_dims(domain_attention, axis=1)\n",
    "    domain_attention = K.expand_dims(domain_attention, axis=1)\n",
    "    x = x * domain_attention\n",
    "\n",
    "    # CBAM (Channel and Spatial Attention)\n",
    "    channels = x.shape[-1]\n",
    "    channel_avg = K.mean(x, axis=(1, 2))\n",
    "    channel_attention = tf.keras.layers.Dense(channels // reduction_ratio, activation='relu')(channel_avg)\n",
    "    channel_attention = tf.keras.layers.Dense(channels, activation='sigmoid')(channel_attention)\n",
    "    channel_attention = K.expand_dims(channel_attention, axis=1)\n",
    "    channel_attention = K.expand_dims(channel_attention, axis=1)\n",
    "    x = x * channel_attention\n",
    "    \n",
    "    # Convolutional Triplet Attention Module\n",
    "    conv_triplet_attention = tf.keras.layers.Conv2D(1, 3, padding='same', activation='sigmoid')(x)\n",
    "    x = x * conv_triplet_attention\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_sparse import SparseTensor\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import add_self_loops\n",
    "from torch_geometric.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import MessagePassing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a sample data object\n",
    "edge_index = torch.tensor([[0, 1, 1, 2], [1, 0, 2, 1]], dtype=torch.long)\n",
    "x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float)\n",
    "data = Data(x=x, edge_index=edge_index)\n",
    "\n",
    "# Define GAT model\n",
    "class SparseGATLayer(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(SparseGATLayer, self).__init__('add')\n",
    "        self.lin = torch.nn.Linear(in_channels, out_channels)\n",
    "        self.att = torch.nn.Linear(2 * out_channels, 1)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.lin(x)\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "        edge_weight = torch.ones(edge_index.size(1))  # Uniform edge weights\n",
    "        edge_index = SparseTensor(row=edge_index[0], col=edge_index[1],\n",
    "                                  value=edge_weight, sparse_sizes=(x.size(0), x.size(0)))\n",
    "        return self.propagate(edge_index, x=x)\n",
    "\n",
    "    def message(self, x_j):\n",
    "        return x_j\n",
    "\n",
    "    def update(self, aggr_out, x):\n",
    "        alpha = self.att(torch.cat([aggr_out, x], dim=1))\n",
    "        alpha = torch.softmax(alpha, dim=1)\n",
    "        x = alpha * x\n",
    "        return self.relu(x)\n",
    "\n",
    "class GATNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GATNet, self).__init__()\n",
    "        self.conv1 = SparseGATLayer(3, 128)\n",
    "        self.conv2 = SparseGATLayer(128, 64)\n",
    "        self.conv3 = SparseGATLayer(64, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Create GAT network\n",
    "model = GATNet()\n",
    "\n",
    "# Convert data to PyTorch DataLoader\n",
    "loader = DataLoader([data], batch_size=1)\n",
    "\n",
    "# Process data using GAT network\n",
    "output = model(data)\n",
    "\n",
    "# Plot input image\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(data.x.numpy())\n",
    "plt.title('Input Image')\n",
    "\n",
    "# Plot output image\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(output.detach().numpy())\n",
    "plt.title('Output Image')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import add_self_loops\n",
    "from torch_geometric.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import MessagePassing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a sample data object\n",
    "edge_index = torch.tensor([[0, 1, 1, 2], [1, 0, 2, 1]], dtype=torch.long)\n",
    "x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float)\n",
    "data = Data(x=x, edge_index=edge_index)\n",
    "\n",
    "# Define GAT model\n",
    "class GATLayer(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GATLayer, self).__init__('add')\n",
    "        self.lin = torch.nn.Linear(in_channels, out_channels)\n",
    "        self.att = torch.nn.Linear(2 * out_channels, 1)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.lin(x)\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "        return self.propagate(edge_index, x=x)\n",
    "\n",
    "    def message(self, x_j, edge_index_i):\n",
    "        return x_j\n",
    "\n",
    "    def update(self, aggr_out, x):\n",
    "        alpha = self.att(torch.cat([aggr_out, x], dim=1))\n",
    "        alpha = torch.softmax(alpha, dim=1)\n",
    "        x = torch.squeeze(torch.matmul(torch.unsqueeze(alpha, 0), x))\n",
    "        return self.relu(x)\n",
    "\n",
    "class GATNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GATNet, self).__init__()\n",
    "        self.conv1 = GATLayer(3, 128)\n",
    "        self.conv2 = GATLayer(128, 64)\n",
    "        self.conv3 = GATLayer(64, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Create GAT network\n",
    "model = GATNet()\n",
    "\n",
    "# Convert data to PyTorch DataLoader\n",
    "loader = DataLoader([data], batch_size=1)\n",
    "\n",
    "# Process data using GAT network\n",
    "output = model(data)\n",
    "\n",
    "# Plot input image\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(data.x.numpy())\n",
    "plt.title('Input Image')\n",
    "\n",
    "# Plot output image\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(output.detach().numpy())\n",
    "plt.title('Output Image')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class SparseGAT(layers.Layer):\n",
    "    def __init__(self, output_dim):\n",
    "        super(SparseGAT, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.fc = layers.Dense(output_dim)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x, edge_index = inputs\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.2)\n",
    "\n",
    "        # Sparse GAT computation\n",
    "        row, col = edge_index[0], edge_index[1]\n",
    "        edge_weight = tf.ones((edge_index.shape[1],), dtype=tf.float32)\n",
    "        edge_weight = tf.SparseTensor(indices=tf.transpose(edge_index),\n",
    "                                      values=edge_weight,\n",
    "                                      dense_shape=(x.shape[0], x.shape[0]))\n",
    "        x = tf.sparse.sparse_dense_matmul(edge_weight, x)\n",
    "\n",
    "        # Channel Attention (CA)\n",
    "        ca = tf.reduce_mean(x, axis=-1, keepdims=True)  # Compute channel-wise mean\n",
    "        ca = self.fc(ca)\n",
    "        ca = tf.nn.sigmoid(ca)\n",
    "        x_ca = tf.multiply(x, ca)  # Apply channel-wise attention\n",
    "\n",
    "        # Spatial Attention (SA)\n",
    "        sa = tf.reduce_mean(x_ca, axis=-1, keepdims=True)  # Compute spatial-wise mean\n",
    "        sa = self.fc(sa)\n",
    "        sa = tf.nn.sigmoid(sa)\n",
    "        x_sa = tf.multiply(x_ca, sa)  # Apply spatial-wise attention\n",
    "\n",
    "        # Patch Attention (PA)\n",
    "        patch_size = 16  # Define the patch size (adjust as needed)\n",
    "        x_reshaped = tf.reshape(x_sa, (-1, patch_size, patch_size, self.output_dim))  # Reshape to patches\n",
    "        pa = self.fc(x_reshaped)\n",
    "        pa = tf.nn.sigmoid(pa)\n",
    "        x_pa = tf.multiply(x_reshaped, pa)  # Apply patch-wise attention\n",
    "        x_pa = tf.reshape(x_pa, (-1, x_pa.shape[1] * x_pa.shape[2], self.output_dim))  # Reshape back to original shape\n",
    "\n",
    "        # Fusion of attention\n",
    "        x_fusion = tf.reduce_sum(x_pa, axis=-1)\n",
    "\n",
    "        return x_fusion\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
